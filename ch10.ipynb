{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first Spark script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary import\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "if not 'sc' in globals():\n",
    "    conf = pyspark.SparkConf()\n",
    "    conf.set('spark.local.dir', 'c:/data')\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "lines = sc.textFile('data/iris.data')\n",
    "linelengths = lines.map(lambda s: len(s))\n",
    "totallength = linelengths.reduce(lambda a,b: a+b)\n",
    "print(totallength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.2', 28)\n",
      "('Iris-setosa', 50)\n",
      "('1.3', 20)\n",
      "('3.1', 12)\n",
      "('1.5', 26)\n",
      "('5.0', 14)\n",
      "('3.6', 4)\n",
      "('5.4', 8)\n",
      "('3.9', 5)\n",
      "('1.7', 6)\n",
      "('0.3', 7)\n",
      "('2.9', 10)\n",
      "('0.1', 6)\n",
      "('4.8', 9)\n",
      "('1.0', 8)\n",
      "('0.5', 1)\n",
      "('1.9', 7)\n",
      "('4.2', 5)\n",
      "('2.3', 12)\n",
      "('0.6', 1)\n",
      "('7.0', 1)\n",
      "('6.4', 8)\n",
      "('6.5', 5)\n",
      "('2.4', 6)\n",
      "('6.6', 3)\n",
      "('2.7', 9)\n",
      "('2.0', 7)\n",
      "('5.9', 5)\n",
      "('6.0', 8)\n",
      "('2.2', 6)\n",
      "('6.7', 10)\n",
      "('6.2', 4)\n",
      "('6.8', 3)\n",
      "('2.6', 5)\n",
      "('Iris-virginica', 50)\n",
      "('7.1', 1)\n",
      "('7.6', 1)\n",
      "('7.3', 1)\n",
      "('7.2', 3)\n",
      "('7.7', 4)\n",
      "('5.1', 17)\n",
      "('3.5', 8)\n",
      "('1.4', 20)\n",
      "('4.9', 11)\n",
      "('3.0', 27)\n",
      "('4.7', 7)\n",
      "('3.2', 13)\n",
      "('4.6', 7)\n",
      "('0.4', 7)\n",
      "('3.4', 12)\n",
      "('4.4', 8)\n",
      "('3.7', 4)\n",
      "('1.6', 11)\n",
      "('4.3', 3)\n",
      "('1.1', 4)\n",
      "('5.8', 10)\n",
      "('4.0', 6)\n",
      "('1.2', 7)\n",
      "('5.7', 11)\n",
      "('3.8', 7)\n",
      "('3.3', 8)\n",
      "('5.2', 6)\n",
      "('4.1', 4)\n",
      "('5.5', 10)\n",
      "('4.5', 9)\n",
      "('5.3', 3)\n",
      "('Iris-versicolor', 50)\n",
      "('6.9', 5)\n",
      "('2.8', 14)\n",
      "('6.3', 10)\n",
      "('6.1', 9)\n",
      "('5.6', 12)\n",
      "('2.5', 11)\n",
      "('1.8', 12)\n",
      "('2.1', 6)\n",
      "('7.4', 1)\n",
      "('7.9', 1)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "if not 'sc' in globals():\n",
    "    sc = pyspark.SparkContext()\n",
    "    \n",
    "lines = sc.textFile('data/iris.data')\n",
    "counts = lines.flatMap(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a+b)\n",
    "    \n",
    "for x in counts.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorted word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.1', 6)\n",
      "('0.2', 28)\n",
      "('0.3', 7)\n",
      "('0.4', 7)\n",
      "('0.5', 1)\n",
      "('0.6', 1)\n",
      "('1.0', 8)\n",
      "('1.1', 4)\n",
      "('1.2', 7)\n",
      "('1.3', 20)\n",
      "('1.4', 20)\n",
      "('1.5', 26)\n",
      "('1.6', 11)\n",
      "('1.7', 6)\n",
      "('1.8', 12)\n",
      "('1.9', 7)\n",
      "('2.0', 7)\n",
      "('2.1', 6)\n",
      "('2.2', 6)\n",
      "('2.3', 12)\n",
      "('2.4', 6)\n",
      "('2.5', 11)\n",
      "('2.6', 5)\n",
      "('2.7', 9)\n",
      "('2.8', 14)\n",
      "('2.9', 10)\n",
      "('3.0', 27)\n",
      "('3.1', 12)\n",
      "('3.2', 13)\n",
      "('3.3', 8)\n",
      "('3.4', 12)\n",
      "('3.5', 8)\n",
      "('3.6', 4)\n",
      "('3.7', 4)\n",
      "('3.8', 7)\n",
      "('3.9', 5)\n",
      "('4.0', 6)\n",
      "('4.1', 4)\n",
      "('4.2', 5)\n",
      "('4.3', 3)\n",
      "('4.4', 8)\n",
      "('4.5', 9)\n",
      "('4.6', 7)\n",
      "('4.7', 7)\n",
      "('4.8', 9)\n",
      "('4.9', 11)\n",
      "('5.0', 14)\n",
      "('5.1', 17)\n",
      "('5.2', 6)\n",
      "('5.3', 3)\n",
      "('5.4', 8)\n",
      "('5.5', 10)\n",
      "('5.6', 12)\n",
      "('5.7', 11)\n",
      "('5.8', 10)\n",
      "('5.9', 5)\n",
      "('6.0', 8)\n",
      "('6.1', 9)\n",
      "('6.2', 4)\n",
      "('6.3', 10)\n",
      "('6.4', 8)\n",
      "('6.5', 5)\n",
      "('6.6', 3)\n",
      "('6.7', 10)\n",
      "('6.8', 3)\n",
      "('6.9', 5)\n",
      "('7.0', 1)\n",
      "('7.1', 1)\n",
      "('7.2', 3)\n",
      "('7.3', 1)\n",
      "('7.4', 1)\n",
      "('7.6', 1)\n",
      "('7.7', 4)\n",
      "('7.9', 1)\n",
      "('Iris-setosa', 50)\n",
      "('Iris-versicolor', 50)\n",
      "('Iris-virginica', 50)\n"
     ]
    }
   ],
   "source": [
    "counts = lines.flatMap(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortByKey()\n",
    "    \n",
    "for x in counts.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.056000\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "\n",
    "def sample(p):\n",
    "    x,y = random.random(), random.random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "    .map(sample) \\\n",
    "    .reduce(lambda a, b: a + b)\n",
    "    \n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log file examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1546 access records\n",
      "1525 GET\n",
      "14 POST\n",
      "7 Others\n"
     ]
    }
   ],
   "source": [
    "textFile = sc.textFile('data/access_log')\n",
    "\n",
    "print(textFile.count(), 'access records')\n",
    "\n",
    "gets = textFile.filter(lambda line: 'GET' in line)\n",
    "print(gets.count(), 'GET')\n",
    "\n",
    "posts = textFile.filter(lambda line: 'POST' in line)\n",
    "print(posts.count(), 'POST')\n",
    "\n",
    "other = textFile.subtract(gets).subtract(posts)\n",
    "print(other.count(), 'Others')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark primes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9592\n"
     ]
    }
   ],
   "source": [
    "def is_it_prime(number):\n",
    "    # make sure n is a positive number\n",
    "    number = abs(int(number))\n",
    "    \n",
    "    # simple test\n",
    "    if number < 2:\n",
    "        return False\n",
    "    \n",
    "    # 2 is prime\n",
    "    if number == 2:\n",
    "        return True\n",
    "    \n",
    "    # other even numbers aren't\n",
    "    if not number & 1:\n",
    "        return False\n",
    "    \n",
    "    # check whether number is divisible into it's square root\n",
    "    for x in range(3, int(number**0.5)+1, 2):\n",
    "        if number % x == 0:\n",
    "            return False\n",
    "    \n",
    "    #if we get this far we are good\n",
    "    return True\n",
    "\n",
    "# create a set of numbers to 100,000\n",
    "numbers = sc.parallelize(range(100000))\n",
    "\n",
    "# count out the number of primes we found\n",
    "print(numbers.filter(is_it_prime).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark text file analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 sentences\n",
      "3463 bigrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(36, ('of', 'the')),\n",
       " (15, ('the', 'mall')),\n",
       " (12, ('to', 'the')),\n",
       " (12, ('on', 'the')),\n",
       " (12, ('At', 'this')),\n",
       " (11, ('the', 'guards')),\n",
       " (11, ('at', 'the')),\n",
       " (11, ('in', 'the')),\n",
       " (9, ('a', 'few')),\n",
       " (9, ('and', 'the'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sc.textFile('data/2600raid.txt') \\\n",
    "    .glom() \\\n",
    "    .map(lambda x: \" \".join(x)) \\\n",
    "    .flatMap(lambda x: x.split(\".\"))\n",
    "\n",
    "print(sentences.count(),\"sentences\")\n",
    "\n",
    "bigrams = sentences.map(lambda x:x.split()) \\\n",
    "    .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\n",
    "    \n",
    "print(bigrams.count(),\"bigrams\")\n",
    "\n",
    "frequent_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \\\n",
    "    .map(lambda x:(x[1],x[0])) \\\n",
    "    .sortByKey(False)\n",
    "\n",
    "frequent_bigrams.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - evaluating history data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2000', 169), ('1999', 166), ('2003', 166), ('2013', 166), ('2010', 165)]\n",
      "[('actor', 596), ('actress', 271), ('journalist', 180), ('author', 102), ('Journalist', 72)]\n",
      "[('Fareed Zakaria', 19), ('Denis Leary', 17), ('Brian Williams', 16), ('Paul Rudd', 13), ('Ricky Gervais', 13)]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import csv\n",
    "import operator\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "years = {}\n",
    "occupations = {}\n",
    "guests = {}\n",
    "\n",
    "#The file header contains these column descriptors\n",
    "#YEAR,GoogleKnowlege_Occupation,Show,Group,Raw_Guest_List\n",
    "with open('data/daily_show_guests.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        year = row['YEAR']\n",
    "        if year in years:\n",
    "            years[year] = years[year] + 1\n",
    "        else:\n",
    "            years[year] = 1\n",
    "            \n",
    "        occupation = row['GoogleKnowlege_Occupation']\n",
    "        if occupation in occupations:\n",
    "            occupations[occupation] = occupations[occupation] + 1\n",
    "        else:\n",
    "            occupations[occupation] = 1\n",
    "            \n",
    "        guest = row['Raw_Guest_List']\n",
    "        if guest in guests:\n",
    "            guests[guest] = guests[guest] + 1\n",
    "        else:\n",
    "            guests[guest] = 1\n",
    "            \n",
    "syears = sorted(years.items(), key=operator.itemgetter(1),reverse=True)\n",
    "soccupations = sorted(occupations.items(),key=operator.itemgetter(1), reverse=True)\n",
    "sguests = sorted(guests.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "print(syears[:5])\n",
    "print(soccupations[:5])\n",
    "print(sguests[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
